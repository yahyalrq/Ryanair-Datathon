{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ryanair Datathon Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.0'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "import sklearn.datasets as datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For this case, we were given two datasets. The first dataset called \"train\" is limited to basic information and it contains the same features as the \"test\" dataset. The second dataset is \"train_extended\" which we will use for the visualisation, as it allows us to detect potential outliers, and brainstorm on new features that we could potentially add to the basic \"train\"  and \"test\" datasets and that would help us in predicting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import all the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv(\"train.csv\")\n",
    "df=pd.read_csv(\"train_extended.csv\")\n",
    "df2=pd.read_csv(\"test.csv\")\n",
    "dfs=[df_train,df,df2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df, title=\"Pandas Profiling Report\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realize that among the variables that are included both in the basic \"train\" dataset and the \"trainextended\" dataset, we have 6 categorical variables, two timeseries variables, and 7 continuous variables including the target variable \"TeledyneRampWeight\". It seems like \"AircraftRegistration\" values are well balanced, however the variables \"AircraftCapacity\", \"AircraftTypegroup\",\"AOC description\", \"Carrier\" and \"Service Description\" are unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.0'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from scipy.stats import norm, skew\n",
    "from scipy import stats\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn \n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Skew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Freight</th>\n",
       "      <td>872.378222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TeledyneRampWeight</th>\n",
       "      <td>4.539957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Infants</th>\n",
       "      <td>2.561255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Children</th>\n",
       "      <td>2.547329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bags</th>\n",
       "      <td>2.003505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BlockTimeScheduled</th>\n",
       "      <td>0.777010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adults</th>\n",
       "      <td>-0.812129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Skew\n",
       "Freight             872.378222\n",
       "TeledyneRampWeight    4.539957\n",
       "Infants               2.561255\n",
       "Children              2.547329\n",
       "Bags                  2.003505\n",
       "BlockTimeScheduled    0.777010\n",
       "Adults               -0.812129"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_feat = ['BlockTimeScheduled','Adults','Children', 'Freight', 'Infants', 'Bags', 'TeledyneRampWeight']\n",
    "feat_skew = df[num_feat].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "skewness = pd.DataFrame({'Skew' : feat_skew})\n",
    "skewness"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "skewed_features = skewness[abs(skewness > 0.75)].index\n",
    "print(skewed_features.values)\n",
    "\n",
    "for feat in skewed_features:\n",
    "    print(feat)\n",
    "    pp = df[feat].value_counts()/df.shape[0]*100.\n",
    "    \n",
    "    sns.distplot(df[feat], fit=norm);\n",
    "    (mu, sigma) = norm.fit(df[feat])\n",
    "    print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "    \n",
    "    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution')\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\n",
    "    res = stats.probplot(df[feat], plot=axs[0])\n",
    "    axs[1] = plt.boxplot(df[feat])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We could detect outliers by comparing values in the dataset with real life possibilities, for example the planes in our dataset \n",
    "\n",
    "cannot exceed 85000kg, and we will actaly proceed to doing that. However the plots give us also valuable information. We can \n",
    "\n",
    "start with freight and detect the presence of outliers, looking at its histogram boxplot and qqplot, and considering that its \n",
    "\n",
    "skewness is of a value 872.3799316 with a mean of 40k a minimum of 0 and a maximum of 22233333. Moving on we can notice the \n",
    "\n",
    "presence of outliers for infants, children and bags. However the distribution of these variables is right skewed, this is \n",
    "\n",
    "actually understandable so this is to be taken into account when performing outliers removal. The same applies for the adults \n",
    "\n",
    "variable even thoguh the distribution is left skewed. The variable BlockTimeScheduled seems to be slightly skewed however we \n",
    "\n",
    "will be able to check its validity from a first a approach just by validating whether it is equal or not to the substraction of \n",
    "\n",
    "the Departure and Ariival Scheduled. Finally, the target variable seems tp have extreme unusual values as we can see in the \n",
    "\n",
    "qqplot and boxplot starting from values 100k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the range of the values for each variable and the sensitivity of each variable, we made the assumption that a regular outlier technique would not well clean the dataset. We validated this assumption through using zscore, IQR, IsolationForest, and Local Outlier Factor. These techniques would clean too much data or they would need to be manually customized for each variable. Therefore we proceeded cleaning the variables as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We started by only selecting the rows in the dataset where the BlockTimeScheduled was higher than the flight time as the opposite is impossible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df[\"FlightTime\"]<df[\"BlockTimeScheduled\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "753819"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Then we selected only the rows where the  TeledyneRampWeight is for sure higher than the basic empty weight of each specific aircraft as the opposite is impossible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa=df[(df['AircraftTypeGroup']==\"NG\") & (df['TeledyneRampWeight']<=41140)].index\n",
    "df=df.drop(dfa)\n",
    "dfb=df[(df['AircraftTypeGroup']==\"Airbus\") & (df['TeledyneRampWeight']<=42600)].index\n",
    "df=df.drop(dfb)\n",
    "dfc=df[(df['AircraftTypeGroup']==\"Max\") & (df['TeledyneRampWeight']<=41140)].index\n",
    "df=df.drop(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "753703"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In order to remove reight outliers, we selected only the rows where the basic empty weight of each particular airplane + freight would not  exceed the aorcraft associated maximum take off weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe= df[(df['AircraftTypeGroup']==\"Max\") & (df['Freight']+45070>82191)].index \n",
    "df=df.drop(dfe)\n",
    "dfg= df[(df['AircraftTypeGroup']==\"NG\") & (df['Freight']+41140>79000)].index \n",
    "df=df.drop(dfg)\n",
    "dff= df[(df['AircraftTypeGroup']==\"Airbus\") &(df['Freight']+42060>77000)].index \n",
    "df=df.drop(dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "753692"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We remove the rows where the ScheduledRoute is different from the Actual route because this behaviour could be due to many reasonsthat we cannot know. Weather, Lack of fuel, overweight, a combination of all... Also because we tried removing them and it performed better. We chose to delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi=df[df[\"ScheduledRoute\"]!=df[\"ActualRoute\"]].index \n",
    "df=df.drop(dfi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "753466"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We deleted the rows where the ramp weight is higher than the planned  takeoffweight because practically the ramp weight should be lower since the fuel weight used for the taxiout is deduced from the ramp weight. After trial, removing these rows performed better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[~(df[\"TeledyneRampWeight\"]>(df[\"PlannedTOW\"]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The rampweight should always be higher than the plannedzero fuel weight considering that the fuelweight is added. However it could happen that the prediction of the zero fuel weight is biased, therefore we put a margin. After trying, dropping these rows with the associated margin performed better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe= df[(df['AircraftTypeGroup']==\"Max\") & (df[\"TeledyneRampWeight\"]<df[\"PlannedZeroFuelWeight\"]-((df[\"PlannedZeroFuelWeight\"]-45070)*10/100))].index \n",
    "df=df.drop(dfe)\n",
    "dfg= df[(df['AircraftTypeGroup']==\"NG\") & (df[\"TeledyneRampWeight\"]<df[\"PlannedZeroFuelWeight\"]-((df[\"PlannedZeroFuelWeight\"]-41140)*10/100))].index \n",
    "df=df.drop(dfg)\n",
    "dff= df[(df['AircraftTypeGroup']==\"Airbus\") & (df['AircraftCapacity']==180) & (df[\"TeledyneRampWeight\"]<df[\"PlannedZeroFuelWeight\"]-((df[\"PlannedZeroFuelWeight\"]-42060)*10/100))].index \n",
    "df=df.drop(dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "618990"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=df['ScheduledRoute'].str.split('-',n = 1, expand = True)\n",
    "df[\"DepartureLocation\"]= test[0]\n",
    "df[\"ArrivalLocation\"]= test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports=pd.read_csv(\"GlobalAirportDatabase.txt\", sep=\",\")\n",
    "airports['longitude']=airports['longitude'].astype(float)\n",
    "airports['latitude']=airports['latitude'].astype(float)\n",
    "\n",
    "for departure in df['DepartureLocation'].unique():\n",
    "    departure_df= df[df['DepartureLocation']==departure]\n",
    "    val= airports[airports['iata']==departure].iloc[:, [-2,-1]].values[0]\n",
    "    departure_df['D_long']=val[1]\n",
    "    departure_df['D_lat']=val[0]\n",
    "    df.loc[df['DepartureLocation']==departure, 'D_long']= val[1]\n",
    "    df.loc[df['DepartureLocation']==departure, 'D_lat']= val[0]\n",
    "\n",
    "    \n",
    "for Arrival in df['ArrivalLocation'].unique():\n",
    "    Arrival_df2= df[df['ArrivalLocation']==Arrival]\n",
    "    val= airports[airports['iata']==Arrival].iloc[:, [-2,-1]].values[0]\n",
    "    Arrival_df2['A_long']=val[1]\n",
    "    Arrival_df2['A_lat']=val[0]\n",
    "    df.loc[df['ArrivalLocation']==Arrival, 'A_long']= val[1]\n",
    "    df.loc[df['ArrivalLocation']==Arrival, 'A_lat']= val[0]\n",
    "\n",
    "R=6371000  \n",
    "lon1,lat1=df['D_long'], df['D_lat']\n",
    "lon2,lat2=df['A_long'],  df['A_lat']                          # radius of Earth in meters\n",
    "phi_1=np.radians(lat1)\n",
    "phi_2=np.radians(lat2)\n",
    "\n",
    "delta_phi=np.radians(lat2-lat1)\n",
    "delta_lambda=np.radians(lon2-lon1)\n",
    "\n",
    "a=np.sin(delta_phi/2.0)**2+\\\n",
    "np.cos(phi_1)*np.cos(phi_2)*\\\n",
    "np.sin(delta_lambda/2.0)**2\n",
    "c=2*np.arctan2(np.sqrt(a),np.sqrt(1-a))\n",
    "\n",
    "meters=R*c                         # output distance in meters\n",
    "km=meters/1000.0              # output distance in kilometers\n",
    "miles=meters*0.000621371      # output distance in miles\n",
    "feet=miles*5280\n",
    "df['distance']= km "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=[\"D_long\",\"D_lat\",\"A_long\",\"A_lat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.get_dummies(df, columns=['ServiceDescription'])\n",
    "\n",
    "df=pd.get_dummies(df, columns=['AircraftTypeGroup'])\n",
    "\n",
    "df=pd.get_dummies(df, columns=['AOCDescription'])\n",
    "\n",
    "df=pd.get_dummies(df, columns=['Carrier'])\n",
    "\n",
    "df=pd.get_dummies(df, columns=['AircraftCapacity'])\n",
    "\n",
    "df=pd.get_dummies(df, columns=['AircraftRegistration'])\n",
    "\n",
    "df=pd.get_dummies(df, columns=['DepartureLocation'])\n",
    "\n",
    "df=pd.get_dummies(df, columns=['ArrivalLocation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=[\"ScheduledRoute\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DepartureScheduled\"]=pd.to_datetime(df[\"DepartureScheduled\"])\n",
    "df[\"ArrivalScheduled\"]=pd.to_datetime(df[\"ArrivalScheduled\"])\n",
    "df[\"DepartureScheduled\"]=pd.to_datetime(df['DepartureScheduled'],format='%y/%m/%d %H:%M:%S')\n",
    "df[\"ArrivalScheduled\"]=pd.to_datetime(df['ArrivalScheduled'],format='%y/%m/%d %H:%M:%S')\n",
    "df['day_of_week'] = df['DepartureScheduled'].dt.day_name()\n",
    "df['hour_of_day'] = df['DepartureScheduled'].dt.hour\n",
    "df['month_of_year'] =df['DepartureScheduled'].dt.month\n",
    "df=df.drop(columns=[\"DepartureScheduled\",\"ArrivalScheduled\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.get_dummies(df, columns=['month_of_year','hour_of_day','day_of_week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns={'PlannedTOW', 'PlannedZeroFuelWeight','TaxiOut','PlannedTripTime','FlightID','FlightNumber','ActualRoute',\n",
    "'DepartureActual','ArrivalActual','BlockTime','TaxiOut','Burnoff','FlightTime'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering for the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Because some of the values in the test dataset variables are not present in the train dataset we will need to apply the feature engineering to the test dataset and make the columns in both datasets match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2[['AircraftRegistration', 'AircraftCapacity',\n",
    "       'AircraftTypeGroup', 'ServiceDescription', 'Carrier', 'AOCDescription',\n",
    "       'ScheduledRoute', 'DepartureScheduled', 'ArrivalScheduled',\n",
    "       'BlockTimeScheduled', 'Adults', 'Children', 'Freight', 'Infants',\n",
    "       'Bags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=df2['ScheduledRoute'].str.split('-',n = 1, expand = True)\n",
    "df2[\"DepartureLocation\"]= test[0]\n",
    "df2[\"ArrivalLocation\"]= test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports=pd.read_csv(\"GlobalAirportDatabase.txt\", sep=\",\")\n",
    "airports['longitude']=airports['longitude'].astype(float)\n",
    "airports['latitude']=airports['latitude'].astype(float)\n",
    "\n",
    "for departure in df2['DepartureLocation'].unique():\n",
    "    departure_df= df2[df2['DepartureLocation']==departure]\n",
    "    val= airports[airports['iata']==departure].iloc[:, [-2,-1]].values[0]\n",
    "    departure_df['D_long']=val[1]\n",
    "    departure_df['D_lat']=val[0]\n",
    "    df2.loc[df2['DepartureLocation']==departure, 'D_long']= val[1]\n",
    "    df2.loc[df2['DepartureLocation']==departure, 'D_lat']= val[0]\n",
    "\n",
    "    \n",
    "for Arrival in df2['ArrivalLocation'].unique():\n",
    "    Arrival_df2= df2[df2['ArrivalLocation']==Arrival]\n",
    "    val= airports[airports['iata']==Arrival].iloc[:, [-2,-1]].values[0]\n",
    "    Arrival_df2['A_long']=val[1]\n",
    "    Arrival_df2['A_lat']=val[0]\n",
    "    df2.loc[df2['ArrivalLocation']==Arrival, 'A_long']= val[1]\n",
    "    df2.loc[df2['ArrivalLocation']==Arrival, 'A_lat']= val[0]\n",
    "\n",
    "R=6371000  \n",
    "lon1,lat1=df2['D_long'], df2['D_lat']\n",
    "lon2,lat2=df2['A_long'],  df2['A_lat']                          # radius of Earth in meters\n",
    "phi_1=np.radians(lat1)\n",
    "phi_2=np.radians(lat2)\n",
    "\n",
    "delta_phi=np.radians(lat2-lat1)\n",
    "delta_lambda=np.radians(lon2-lon1)\n",
    "\n",
    "a=np.sin(delta_phi/2.0)**2+\\\n",
    "np.cos(phi_1)*np.cos(phi_2)*\\\n",
    "np.sin(delta_lambda/2.0)**2\n",
    "c=2*np.arctan2(np.sqrt(a),np.sqrt(1-a))\n",
    "\n",
    "meters=R*c                         # output distance in meters\n",
    "km=meters/1000.0              # output distance in kilometers\n",
    "miles=meters*0.000621371      # output distance in miles\n",
    "feet=miles*5280\n",
    "df2['distance']= km "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.get_dummies(df2, columns=['ServiceDescription'])\n",
    "df2=pd.get_dummies(df2, columns=['AircraftTypeGroup'])\n",
    "df2=pd.get_dummies(df2,columns=['AOCDescription'])\n",
    "df2=pd.get_dummies(df2,columns=['Carrier'])\n",
    "df2=pd.get_dummies(df2,columns=['AircraftCapacity'])\n",
    "df2=pd.get_dummies(df2,columns=['DepartureLocation'])\n",
    "df2=pd.get_dummies(df2,columns=['ArrivalLocation'])\n",
    "df2=pd.get_dummies(df2,columns=[\"AircraftRegistration\"])\n",
    "df2=df2.drop(columns=[\"ScheduledRoute\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"DepartureScheduled\"]=pd.to_datetime(df2[\"DepartureScheduled\"])\n",
    "df2[\"ArrivalScheduled\"]=pd.to_datetime(df2[\"ArrivalScheduled\"])\n",
    "df2[\"DepartureScheduled\"]=pd.to_datetime(df2['DepartureScheduled'],format='%y/%m/%d %H:%M:%S')\n",
    "df2['day_of_week'] = df2['DepartureScheduled'].dt.day_name()\n",
    "df2['hour_of_day'] = df2['DepartureScheduled'].dt.hour\n",
    "df2['month_of_year'] =df2['DepartureScheduled'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.drop(columns={'DepartureScheduled', 'ArrivalScheduled'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.get_dummies(df2, columns=['month_of_year','hour_of_day','day_of_week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.drop(columns=[\"D_long\",\"D_lat\",\"A_long\",\"A_lat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop(columns=[\"TeledyneRampWeight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AircraftRegistration_EIFOH\n",
      "DepartureLocation_LLA\n",
      "DepartureLocation_SPC\n",
      "DepartureLocation_SPU\n",
      "ArrivalLocation_LYS\n",
      "ArrivalLocation_VBY\n"
     ]
    }
   ],
   "source": [
    "for i in x.columns:\n",
    "    if i not in df2.columns:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DepartureLocation_VBY\n",
      "ArrivalLocation_PRN\n"
     ]
    }
   ],
   "source": [
    "for i in df2.columns:\n",
    "    if i not in x.columns:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in x.columns:\n",
    "    if i not in df2.columns:\n",
    "        df2[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df2.columns:\n",
    "    if i not in x.columns:\n",
    "         x[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reindex(sorted(x.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.reindex(sorted(x.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[\"TeledyneRampWeight\"]=df[\"TeledyneRampWeight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(618990, 1282)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(191309, 1281)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.layers as layers\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS MODEL GAVE A MIN SCORE ON RTT OF 591 AND ON KAGGLE 935\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        layers.Input(shape=(1282)),\n",
    "        layers.Dense(642, activation='relu'),\n",
    "        layers.Dense(321, activation='relu'),\n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', \n",
    "                       loss='mean_absolute_error', \n",
    "                       metrics=[tf.keras.metrics.MeanAbsoluteError()]\n",
    "                      )\n",
    "    return model\n",
    "model=create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(\"N3/Mod1/NN3model_{epoch:02d}_{val_loss:.2f}\", \n",
    "                                                      monitor = \"val_mean_absolute_error\",\n",
    "                                                      save_weights_only=True)\n",
    "callbacks = [ checkpointer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"TeledyneRampWeight\"])\n",
    "y = df[\"TeledyneRampWeight\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model was trained for 200 epochs and we chose the epoch 120 to make the predictions\n",
    "history = model.fit(x=X_train.values, y=Y_train.values, \n",
    "              epochs=200, verbose=2, callbacks=callbacks,\n",
    "              validation_data = (X_test.values, y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint('N3/Mod2'))\n",
    "outputs_predicted = model.predict(rt)\n",
    "outputs_predicted.shape\n",
    "from sklearn import metrics\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(rtt[\"TeledyneRampWeight\"], outputs_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_predicted = model.predict(df2)\n",
    "df_submission_file = pd.read_csv(\"test.csv\")\n",
    "df_submission_file[\"TeledyneRampWeight\"]=outputs_predicted\n",
    "df_submission_file =df_submission_file[[\"FlightID\", \"TeledyneRampWeight\"]]\n",
    "df_submission_file=df_submission_file.set_index(\"FlightID\")\n",
    "df_submission_file\n",
    "df_submission_file.to_csv(\"predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
